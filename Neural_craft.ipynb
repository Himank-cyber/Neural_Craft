{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366376a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "class Config:\n",
    "    MAX_LEN = 512           \n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    VALID_BATCH_SIZE = 16\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 3e-5\n",
    "    BERT_MODEL_NAME = 'bert-base-uncased' \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {Config.device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Remove the \"XXXX\" redaction placeholders\n",
    "    text = re.sub(r'X{2,}', '', text) \n",
    "\n",
    "    # 2. Standardize money format {$100.00} -> money_token\n",
    "    text = re.sub(r'\\{\\$[\\d,]+\\.?\\d*\\}', '[MONEY]', text)\n",
    "    \n",
    "    # 3. Remove non-alphanumeric characters (keep basic punctuation)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.,!?]', '', text)\n",
    "\n",
    "    # 4. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "df_train = pd.read_csv(\"/kaggle/input/neural-craft/train_complaints.csv\")\n",
    "df_test = pd.read_csv(\"/kaggle/input/neural-craft/test_complaints.csv\")\n",
    "\n",
    "# Apply Cleaning\n",
    "df_train['clean_text'] = df_train['complaint_text'].apply(clean_text)\n",
    "df_test['clean_text'] = df_test['complaint_text'].apply(clean_text)\n",
    "\n",
    "# --- Label Encoding---\n",
    "# We need to map text categories to numbers (0, 1, 2...)\n",
    "primary_encoder = LabelEncoder()\n",
    "secondary_encoder = LabelEncoder()\n",
    "df_train['primary_label'] = primary_encoder.fit_transform(df_train['primary_category'])\n",
    "df_train['secondary_label'] = secondary_encoder.fit_transform(df_train['secondary_category'])\n",
    "\n",
    "# Severity is already numeric (1-5), but for regression we ensure it's float\n",
    "df_train['severity'] = df_train['severity'].astype(float)\n",
    "\n",
    "# Save number of classes for the model architecture\n",
    "NUM_PRIMARY_LABELS = len(primary_encoder.classes_)\n",
    "NUM_SECONDARY_LABELS = len(secondary_encoder.classes_)\n",
    "\n",
    "print(f\"Primary Categories: {NUM_PRIMARY_LABELS}\")\n",
    "print(f\"Secondary Categories: {NUM_SECONDARY_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40812343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplaintDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, is_test=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "        self.text = df['clean_text'].values\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.primary = df['primary_label'].values\n",
    "            self.secondary = df['secondary_label'].values\n",
    "            self.severity = df['severity'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "        if not self.is_test:\n",
    "            item['primary_labels'] = torch.tensor(self.primary[index], dtype=torch.long)\n",
    "            item['secondary_labels'] = torch.tensor(self.secondary[index], dtype=torch.long)\n",
    "            # Severity is float for Regression (MSE Loss)\n",
    "            item['severity_labels'] = torch.tensor(self.severity[index], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, n_primary, n_secondary):\n",
    "        super(MultiTaskBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(Config.BERT_MODEL_NAME)\n",
    "        \n",
    "        # INCREASED CAPACITY: Add an extra dense layer\n",
    "        self.pre_classifier = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier_dropout = nn.Dropout(p=0.3)\n",
    "        self.activation = nn.Tanh() \n",
    "        \n",
    "        # Head 1: Primary\n",
    "        self.out_primary = nn.Linear(self.bert.config.hidden_size, n_primary)\n",
    "        \n",
    "        # Head 2: Secondary\n",
    "        self.out_secondary = nn.Linear(self.bert.config.hidden_size, n_secondary)\n",
    "        \n",
    "        # Head 3: Severity\n",
    "        self.out_severity = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooler_output = output.pooler_output\n",
    "        \n",
    "        # Pass through the new intermediate layer\n",
    "        hidden_state = self.pre_classifier(pooler_output)\n",
    "        hidden_state = self.activation(hidden_state)\n",
    "        hidden_state = self.classifier_dropout(hidden_state)\n",
    "        \n",
    "        # Final predictions\n",
    "        primary_logits = self.out_primary(hidden_state)\n",
    "        secondary_logits = self.out_secondary(hidden_state)\n",
    "        severity_logits = self.out_severity(hidden_state)\n",
    "        \n",
    "        return primary_logits, secondary_logits, severity_logits\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # ===Apply Class Weights here ===\n",
    "    loss_fct_prim = nn.CrossEntropyLoss(weight=weights_prim)\n",
    "    loss_fct_sec = nn.CrossEntropyLoss(weight=weights_sec)\n",
    "    \n",
    "    # Severity is regression, so we don't use class weights, \n",
    "    # but we can multiply the loss to make the model focus on it more.\n",
    "    loss_fct_sev = nn.MSELoss()\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        primary_targets = d[\"primary_labels\"].to(device)\n",
    "        secondary_targets = d[\"secondary_labels\"].to(device)\n",
    "        severity_targets = d[\"severity_labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        o_prim, o_sec, o_sev = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Calculate individual losses\n",
    "        loss_prim = loss_fct_prim(o_prim, primary_targets)\n",
    "        loss_sec = loss_fct_sec(o_sec, secondary_targets)\n",
    "        loss_sev = loss_fct_sev(o_sev.view(-1), severity_targets)\n",
    "        \n",
    "        # === TUNING THE COMBINED LOSS ===\n",
    "        # We assume Secondary is hardest, so we might give it slightly more weight \n",
    "        loss = (1.0 * loss_prim) + (2.0 * loss_sec) + (1.0 * loss_sev)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (Prevents \"exploding gradients\" which ruin accuracy)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets_prim = []\n",
    "    fin_outputs_prim = []\n",
    "    fin_targets_sec = []\n",
    "    fin_outputs_sec = []\n",
    "    fin_targets_sev = []\n",
    "    fin_outputs_sev = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            p_t = d[\"primary_labels\"].cpu().detach().numpy()\n",
    "            s_t = d[\"secondary_labels\"].cpu().detach().numpy()\n",
    "            sev_t = d[\"severity_labels\"].cpu().detach().numpy()\n",
    "            o_prim, o_sec, o_sev = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # For classification, get the index of max logit\n",
    "            prim_preds = torch.argmax(o_prim, dim=1).cpu().detach().numpy()\n",
    "            sec_preds = torch.argmax(o_sec, dim=1).cpu().detach().numpy()\n",
    "\n",
    "            # For regression, keep raw float\n",
    "            sev_preds = o_sev.view(-1).cpu().detach().numpy()\n",
    "            fin_targets_prim.extend(p_t)\n",
    "            fin_outputs_prim.extend(prim_preds)\n",
    "            fin_targets_sec.extend(s_t)\n",
    "            fin_outputs_sec.extend(sec_preds)\n",
    "            fin_targets_sev.extend(sev_t)\n",
    "            fin_outputs_sev.extend(sev_preds)\n",
    "\n",
    "    return (fin_outputs_prim, fin_targets_prim, \n",
    "            fin_outputs_sec, fin_targets_sec, \n",
    "            fin_outputs_sev, fin_targets_sev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate weights for Primary Category\n",
    "class_weights_prim = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(df_train['primary_label']), \n",
    "    y=df_train['primary_label']\n",
    ")\n",
    "\n",
    "# Convert to Tensor\n",
    "weights_prim = torch.tensor(class_weights_prim, dtype=torch.float).to(Config.device)\n",
    "\n",
    "# Calculate weights for Secondary Category\n",
    "class_weights_sec = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(df_train['secondary_label']), \n",
    "    y=df_train['secondary_label']\n",
    ")\n",
    "\n",
    "# Convert to Tensor\n",
    "weights_sec = torch.tensor(class_weights_sec, dtype=torch.float).to(Config.device)\n",
    "print(\"Class weights calculated and moved to device.\")\n",
    "\n",
    "# Split Train into Train/Validation\n",
    "df_train_split, df_val_split = train_test_split(df_train, test_size=0.1, random_state=42)\n",
    "tokenizer = BertTokenizer.from_pretrained(Config.BERT_MODEL_NAME)\n",
    "train_dataset = ComplaintDataset(df_train_split, tokenizer, Config.MAX_LEN)\n",
    "valid_dataset = ComplaintDataset(df_val_split, tokenizer, Config.MAX_LEN)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=Config.VALID_BATCH_SIZE)\n",
    "\n",
    "# --- Setup ---\n",
    "best_score = -float('inf')\n",
    "best_model_path = \"best_multitask_bert.bin\"\n",
    "\n",
    "# Initialize Model\n",
    "model = MultiTaskBERT(n_primary=NUM_PRIMARY_LABELS, n_secondary=NUM_SECONDARY_LABELS)\n",
    "model = model.to(Config.device)\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "num_train_steps = int(len(df_train_split) / Config.TRAIN_BATCH_SIZE * Config.EPOCHS)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=Config.LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "\n",
    "# --- Training Loop ---\n",
    "\n",
    "print(f\"Starting training for {Config.EPOCHS} epochs...\")\n",
    "for epoch in range(Config.EPOCHS):\n",
    "    # 1. Train\n",
    "    train_loss = train_fn(train_data_loader, model, optimizer, Config.device, scheduler)\n",
    "\n",
    "    # 2. Validate\n",
    "    o_prim, t_prim, o_sec, t_sec, o_sev, t_sev = eval_fn(valid_data_loader, model, Config.device)\n",
    "\n",
    "    # 3. Calculate Metrics\n",
    "    acc_prim = accuracy_score(t_prim, o_prim)\n",
    "    acc_sec = accuracy_score(t_sec, o_sec)\n",
    "    r2_sev = r2_score(t_sev, o_sev)\n",
    "    \n",
    "    final_score = (0.3 * acc_prim) + (0.4 * acc_sec) + (0.3 * r2_sev)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{Config.EPOCHS} | Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Scores -> Primary: {acc_prim:.4f} | Secondary: {acc_sec:.4f} | Severity R2: {r2_sev:.4f}\")\n",
    "    print(f\"Weighted Score: {final_score:.4f}\")\n",
    "    \n",
    "    # 4. Save Logic: If this score is better than the best, save it\n",
    "    if final_score > best_score:\n",
    "        print(f\"--> Improvement detected! Saving model (Old Best: {best_score:.4f} -> New Best: {final_score:.4f})\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        best_score = final_score\n",
    "\n",
    "    else:\n",
    "        print(f\"No improvement (Best is still: {best_score:.4f})\")\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(f\"\\nTraining Complete. Best Weighted Score achieved: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f44af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the Best Model ---\n",
    "\n",
    "print(\"Loading the best saved model for inference...\")\n",
    "\n",
    "model = MultiTaskBERT(n_primary=NUM_PRIMARY_LABELS, n_secondary=NUM_SECONDARY_LABELS)\n",
    "model.load_state_dict(torch.load(\"best_multitask_bert.bin\"))\n",
    "model = model.to(Config.device)\n",
    "model.eval()\n",
    "\n",
    "# --- Setup Test Data ---\n",
    "test_dataset = ComplaintDataset(df_test, tokenizer, Config.MAX_LEN, is_test=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=Config.VALID_BATCH_SIZE, shuffle=False)\n",
    "predictions_prim = []\n",
    "predictions_sec = []\n",
    "predictions_sev = []\n",
    "# --- Predict ---\n",
    "with torch.no_grad():\n",
    "    for d in test_data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(Config.device)\n",
    "        attention_mask = d[\"attention_mask\"].to(Config.device)\n",
    "        o_prim, o_sec, o_sev = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Classification: Get Max Logit\n",
    "        predictions_prim.extend(torch.argmax(o_prim, dim=1).cpu().detach().numpy())\n",
    "        predictions_sec.extend(torch.argmax(o_sec, dim=1).cpu().detach().numpy())\n",
    "        \n",
    "        # Regression: Get Raw Float\n",
    "        predictions_sev.extend(o_sev.view(-1).cpu().detach().numpy())\n",
    "# --- Inverse Transform Labels ---\n",
    "final_prim_labels = primary_encoder.inverse_transform(predictions_prim)\n",
    "final_sec_labels = secondary_encoder.inverse_transform(predictions_sec)\n",
    "# --- Post-process Severity ---\n",
    "# Round to nearest integer (1.9 -> 2) and clip to range [1, 5]\n",
    "final_sev_labels = [int(max(1, min(5, round(x)))) for x in predictions_sev]\n",
    "\n",
    "# --- Create Submission ---\n",
    "submission = pd.DataFrame({\n",
    "    'complaint_id': df_test['complaint_id'],\n",
    "    'complaint_text': df_test['complaint_text'],\n",
    "    'primary_category': final_prim_labels,\n",
    "    'secondary_category': final_sec_labels,\n",
    "    'severity': final_sev_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_best_model.csv\", index=False)\n",
    "print(\"Submission file 'submission_best_model.csv' created successfully using the best model.\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
